# Multi-Agent Racing with Self-Play Reinforcement Learning

A 'from scratch' implementation of a self-playing PPO and Gymnasium environment, compared against single-agent PPO (from scratch and SB3).

## Project Overview

This project showcases a complete RL pipeline from environment design to multi-agent training, featuring:

- **Custom racing environment** built from scratch using Gymnasium
- **PPO implementation** from scratch 
- **Self-play training** where agents learn to race competitively against past versions of themselves
- **Multi-agent dynamics** with collision detection and competitive reward structures

https://github.com/user-attachments/assets/6e7963f1-34c4-4d83-b6cc-06ab8ca47f73

## ðŸ“Š Results

![Evaluation Comparison](static/eval_comparison.png)

**Performance Metrics (Successful Runs Only):**
- **Success Rate**: % of races completed
- **Average Speed**: Racing velocity in successful runs
- **Average Distance**: Total path length (lower = tighter racing line)

**Efficiency Metrics (All Runs, Including Crashes):**
- **Steps/Progress**: Time efficiency (lower = faster completion)
- **Distance/Progress**: Path efficiency (lower = optimal racing line)

![Training Comparison](static/training_eval.png)

## Architecture

### Environment Design

**Observation Space (19,):**
- 11 raycasted distance sensors (180Â° front cone)
- Forward/lateral velocity, angular velocity, steering angle
- 4 relative features per opponent (position & velocity in local frame)

**Action Space (2,):**
- Steering: [-1, 1] (full left to full right)
- Throttle: [0, 1] (no acceleration to full throttle)

### PPO Implementation

**Key Features:**
- Generalized Advantage Estimation (GAE)
- Learning rate annealing
- Log-std annealing for exploration â†’ exploitation
- Gradient clipping and advantage normalization
- KL divergence early stopping

### Self-Play Training

**Algorithm:**
1. Train agent vs random opponent (updates 1-15)
2. Create snapshot every 15 updates
3. Maintain opponent pool (max 5 snapshots)
4. Each rollout samples random opponent from pool

## Installation
```bash
# Clone repository
git clone https://github.com/yourusername/racing-self-play
cd racing-self-play

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Project Structure
```
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ ppo.py              # PPO implementation from scratch
â”‚   â””â”€â”€ self_play_ppo.py    # Self-play wrapper with opponent pool
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.py      # Base hyperparameters
â”‚   â””â”€â”€ self_play_config.py # Self-play specific config
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ car.py              # Vehicle physics
â”‚   â”œâ”€â”€ multi_car.py        # Multi-agent car handling
â”‚   â”œâ”€â”€ multi_racing_env.py # Multi-agent racing environment
â”‚   â”œâ”€â”€ multi_track.py      # Multi-agent track handling
â”‚   â”œâ”€â”€ racing_env.py       # Single-agent racing environment
â”‚   â”œâ”€â”€ track.py            # Procedural track generation & boundary logic
â”‚   â””â”€â”€ wrappers.py         # Self-play opponent wrapper
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py          # Evaluation metrics
â”‚   â””â”€â”€ visualization.py    # Racing visualization & video generation
â”œâ”€â”€ static/                 # Generated visualizations & metrics
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ evaluate.py             # Evaluation script
â””â”€â”€ README.md
```

## References

- [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) - Schulman et al. 2017
- [Emergent Complexity via Multi-Agent Competition](https://arxiv.org/abs/1710.03748) - Bansal et al. 2017
- [Mastering Atari with Self-Play](https://arxiv.org/abs/1809.09988) - OpenAI Five
- [Stable Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Hugging Face DRL](https://huggingface.co/learn/deep-rl-course/en/unit8/introduction)

## License

MIT License
